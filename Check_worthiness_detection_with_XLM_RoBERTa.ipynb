{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gr8Gxsv3CrJL",
        "ZzLtZsleCwhT"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Almamun809/Daily-NLP/blob/main/Check_worthiness_detection_with_XLM_RoBERTa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intstalling Huggingface Transformers, Datasets, and Evaluate"
      ],
      "metadata": {
        "id": "Al7KNNl-FJHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBGVNisxFIJE",
        "outputId": "89d2d2aa-56fe-4391-aa40-7d0af529e002"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.0-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Collecting aiohttp (from datasets)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Collecting responses<0.19 (from datasets)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.22.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.4.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2022.7.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/Colab Notebooks/clef-23/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPEes24GEa0X",
        "outputId": "c157cfd7-75eb-49fd-f0bb-43274a5a601f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Colab Notebooks/clef-23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/clef-23/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8E8HqEJ6le-M",
        "outputId": "ae0f7ab5-a9bb-4dc8-f340-e02abb90cbb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/clef-23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenize function to tokenize the input data"
      ],
      "metadata": {
        "id": "gr8Gxsv3CrJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cc62CjYiFFkM",
        "outputId": "8478fa2e-4ef9-43e9-8de3-44e1d70a4d17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "def nltk_tokenization(text):\n",
        "  \"\"\"\n",
        "  text: raw text\n",
        "  return a string after tokenization\n",
        "  \"\"\"\n",
        "  text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text).split())\n",
        "  return \" \".join(word_tokenize(text))\n"
      ],
      "metadata": {
        "id": "-bGZvPC28H68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Read data and Tokenize the data"
      ],
      "metadata": {
        "id": "ZzLtZsleCwhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def merge_train_en(files:list):\n",
        "  data = []\n",
        "  header = ['text', 'label']\n",
        "  for file in files:\n",
        "    with open(file, 'r', encoding='utf8') as f:\n",
        "      reader = csv.reader(f, delimiter='\\t')\n",
        "      next(reader)\n",
        "      for row in reader:\n",
        "        sentence_id = row[0]\n",
        "        text = row[1]\n",
        "        text = nltk_tokenization(text)\n",
        "        label = 1 if row[2] == 'Yes' else 0\n",
        "        data.append([text, label])\n",
        "  with open('./data/1b_en/train.csv', 'w') as f:\n",
        "    writer = csv.writer(f, delimiter=',')\n",
        "    writer.writerow(header)\n",
        "    for row in data:\n",
        "      writer.writerow(row)\n",
        "\n",
        "def prep_dev(in_file, delim=','):\n",
        "  print(f\"Reading file {in_file}.......\")\n",
        "  output_data = []\n",
        "  header = ['text', 'label']\n",
        "  with open(in_file, 'r') as f:\n",
        "    reader = csv.reader(f, delimiter=delim)\n",
        "    next(reader)\n",
        "    for row in reader:\n",
        "      if len(row) >=3:\n",
        "        tokenized_text = nltk_tokenization(row[1])\n",
        "        output_data.append([tokenized_text, 1 if row[2] == 'Yes' else 0])\n",
        "  with open('./data/1b_en/dev.csv', 'w') as f:\n",
        "    writer = csv.writer(f, delimiter=',')\n",
        "    writer.writerow(header)\n",
        "    for row in output_data:\n",
        "      writer.writerow(row)\n",
        "\n",
        "def prep_test(in_file, delim=','):\n",
        "  print(f\"Reading file {in_file}.......\")\n",
        "  output_data = []\n",
        "  header = ['text', 'label']\n",
        "  with open(in_file, 'r') as f:\n",
        "    reader = csv.reader(f, delimiter=delim)\n",
        "    next(reader)\n",
        "    for row in reader:\n",
        "      tokenized_text = nltk_tokenization(row[1])\n",
        "      output_data.append([tokenized_text, 1])\n",
        "  with open('./data/1b_en/test.csv', 'w') as f:\n",
        "    writer = csv.writer(f, delimiter=',')\n",
        "    writer.writerow(header)\n",
        "    for row in output_data:\n",
        "      writer.writerow(row)\n",
        "\n",
        "\n",
        "merge_train_en(['./data/1b_en/CT23_1B_checkworthy_spanish_train.tsv', './data/1b_en/CT23_1B_checkworthy_spanish_dev_test.tsv'])\n",
        "prep_dev('./data/1b_en/CT23_1B_checkworthy_spanish_dev.tsv', '\\t')\n",
        "prep_test('./data/1b_en/CT23_1B_checkworthy_spanish_test.tsv', '\\t')\n",
        "\n",
        "\"\"\"\n",
        "  with open(file2, 'r', encoding='utf8') as f:\n",
        "    header = ['text', 'label']\n",
        "    reader = csv.reader(f, delimiter='\\t')\n",
        "    next(reader)\n",
        "    for row in reader:\n",
        "      sentence_id = row[0]\n",
        "      text = row[1]\n",
        "      label = row[2]\n",
        "      data.append([text, label])\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "MsB_Wa6xKgvs",
        "outputId": "dc99c670-2c4d-4d4e-e984-7dc97d1e9167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading file ./data/1b_en/CT23_1B_checkworthy_spanish_dev.tsv.......\n",
            "Reading file ./data/1b_en/CT23_1B_checkworthy_spanish_test.tsv.......\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n  with open(file2, 'r', encoding='utf8') as f:\\n    header = ['text', 'label']\\n    reader = csv.reader(f, delimiter='\\t')\\n    next(reader)\\n    for row in reader:\\n      sentence_id = row[0]\\n      text = row[1]\\n      label = row[2]\\n      data.append([text, label])\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpaWhkHcLIv_",
        "outputId": "b27e6034-c4ee-443c-e9f2-9c294623d529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save the toeknized data"
      ],
      "metadata": {
        "id": "S_ETIokcC8P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_glue_v1.py \\\n",
        "  --model_name_or_path xlm-roberta-large \\\n",
        "  --train_file ./data/1B_AR/train.csv \\\n",
        "  --validation_file ./data/1B_AR/dev.csv \\\n",
        "  --test_file ./data/1B_AR/test.csv \\\n",
        "  --do_predict \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 5 \\\n",
        "  --output_dir ./xlm-roberta-large-output-AR/ \\\n",
        "  --do_eval \\\n",
        "  --do_train \\\n",
        "  --overwrite_output_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNKlO2KdOUj6",
        "outputId": "03359d90-86cf-45ab-ffe3-87a199317fed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-11 04:58:31.653188: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Edited v1.0.23\n",
            "05/11/2023 04:58:33 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
            "05/11/2023 04:58:33 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./xlm-roberta-large-output-AR/runs/May11_04-58-33_942c0bccd40b,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=./xlm-roberta-large-output-AR/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./xlm-roberta-large-output-AR/,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "05/11/2023 04:58:33 - INFO - __main__ - load a local file for train: ./data/1B_AR/train.csv\n",
            "05/11/2023 04:58:33 - INFO - __main__ - load a local file for validation: ./data/1B_AR/dev.csv\n",
            "05/11/2023 04:58:33 - INFO - __main__ - load a local file for test: ./data/1B_AR/test.csv\n",
            "05/11/2023 04:58:33 - INFO - datasets.builder - Using custom data configuration default-1bb687a002746e7b\n",
            "05/11/2023 04:58:33 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/csv\n",
            "05/11/2023 04:58:33 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-1bb687a002746e7b/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-1bb687a002746e7b/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 3327.05it/s]\n",
            "05/11/2023 04:58:33 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "05/11/2023 04:58:33 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 3/3 [00:01<00:00,  2.65it/s]\n",
            "05/11/2023 04:58:35 - INFO - datasets.builder - Generating train split\n",
            "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/datasets/\u001b[0m\u001b[1;33mbuilder.py\u001b[0m:\u001b[94m1873\u001b[0m in          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_prepare_split_single\u001b[0m                                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1870 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0mstorage_options=\u001b[96mself\u001b[0m._fs.storage_options, \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1871 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0membed_local_files=embed_local_files,      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1872 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m)                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1873 \u001b[2m│   │   │   │   │   \u001b[0mwriter.write_table(table)                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1874 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mnum_examples_progress_update += \u001b[96mlen\u001b[0m(table)        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1875 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mif\u001b[0m time.time() > _time + config.PBAR_REFRESH_TIME \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1876 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m_time = time.time()                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/datasets/\u001b[0m\u001b[1;33marrow_writer.py\u001b[0m:\u001b[94m568\u001b[0m in      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mwrite_table\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m565 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.pa_writer \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m566 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._build_writer(inferred_schema=pa_table.schema)        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m567 \u001b[0m\u001b[2m│   │   \u001b[0mpa_table = pa_table.combine_chunks()                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m568 \u001b[2m│   │   \u001b[0mpa_table = table_cast(pa_table, \u001b[96mself\u001b[0m._schema)                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m569 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.embed_local_files:                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m570 \u001b[0m\u001b[2m│   │   │   \u001b[0mpa_table = embed_table_storage(pa_table)                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m571 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._num_bytes += pa_table.nbytes                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/datasets/\u001b[0m\u001b[1;33mtable.py\u001b[0m:\u001b[94m2290\u001b[0m in \u001b[92mtable_cast\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2287 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33mtable (`pyarrow.Table`): the casted table\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2288 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2289 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m table.schema != schema:                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2290 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m cast_table_to_schema(table, schema)                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2291 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melif\u001b[0m table.schema.metadata != schema.metadata:                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2292 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m table.replace_schema_metadata(schema.metadata)         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2293 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melse\u001b[0m:                                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/datasets/\u001b[0m\u001b[1;33mtable.py\u001b[0m:\u001b[94m2249\u001b[0m in            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mcast_table_to_schema\u001b[0m                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2246 \u001b[0m\u001b[2m│   \u001b[0mfeatures = Features.from_arrow_schema(schema)                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2247 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m \u001b[96msorted\u001b[0m(table.column_names) != \u001b[96msorted\u001b[0m(features):                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2248 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mCouldn\u001b[0m\u001b[33m'\u001b[0m\u001b[33mt cast\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m{\u001b[0mtable.schema\u001b[33m}\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33mto\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m{\u001b[0mfeatur \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2249 \u001b[2m│   \u001b[0marrays = [cast_array_to_feature(table[name], feature) \u001b[94mfor\u001b[0m name, f \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2250 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m pa.Table.from_arrays(arrays, schema=schema)                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2251 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2252 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/datasets/\u001b[0m\u001b[1;33mtable.py\u001b[0m:\u001b[94m2249\u001b[0m in \u001b[92m<listcomp>\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2246 \u001b[0m\u001b[2m│   \u001b[0mfeatures = Features.from_arrow_schema(schema)                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2247 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m \u001b[96msorted\u001b[0m(table.column_names) != \u001b[96msorted\u001b[0m(features):                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2248 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mCouldn\u001b[0m\u001b[33m'\u001b[0m\u001b[33mt cast\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m{\u001b[0mtable.schema\u001b[33m}\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33mto\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m{\u001b[0mfeatur \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2249 \u001b[2m│   \u001b[0marrays = [cast_array_to_feature(table[name], feature) \u001b[94mfor\u001b[0m name, f \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2250 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m pa.Table.from_arrays(arrays, schema=schema)                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2251 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2252 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/datasets/\u001b[0m\u001b[1;33mtable.py\u001b[0m:\u001b[94m1817\u001b[0m in \u001b[92mwrapper\u001b[0m    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1814 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1815 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mwrapper\u001b[0m(array, *args, **kwargs):                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1816 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(array, pa.ChunkedArray):                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1817 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m pa.chunked_array([func(chunk, *args, **kwargs) \u001b[94mfor\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1818 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1819 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m func(array, *args, **kwargs)                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1820 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/datasets/\u001b[0m\u001b[1;33mtable.py\u001b[0m:\u001b[94m1817\u001b[0m in \u001b[92m<listcomp>\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1814 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1815 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mwrapper\u001b[0m(array, *args, **kwargs):                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1816 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(array, pa.ChunkedArray):                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1817 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m pa.chunked_array([func(chunk, *args, **kwargs) \u001b[94mfor\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1818 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1819 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m func(array, *args, **kwargs)                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1820 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/datasets/\u001b[0m\u001b[1;33mtable.py\u001b[0m:\u001b[94m2109\u001b[0m in            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mcast_array_to_feature\u001b[0m                                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2106 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m pa.types.is_null(array.type):                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2107 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m array_cast(array, get_nested_type(feature), allow_numb \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2108 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96misinstance\u001b[0m(feature, (Sequence, \u001b[96mdict\u001b[0m, \u001b[96mlist\u001b[0m, \u001b[96mtuple\u001b[0m)):      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2109 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m array_cast(array, feature(), allow_number_to_str=allow \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2110 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mTypeError\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mCouldn\u001b[0m\u001b[33m'\u001b[0m\u001b[33mt cast array of type\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m{\u001b[0marray.type\u001b[33m}\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33mto\u001b[0m\u001b[33m\\n\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2111 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2112 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/datasets/\u001b[0m\u001b[1;33mtable.py\u001b[0m:\u001b[94m1819\u001b[0m in \u001b[92mwrapper\u001b[0m    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1816 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(array, pa.ChunkedArray):                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1817 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m pa.chunked_array([func(chunk, *args, **kwargs) \u001b[94mfor\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1818 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1819 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m func(array, *args, **kwargs)                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1820 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1821 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m wrapper                                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1822 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/datasets/\u001b[0m\u001b[1;33mtable.py\u001b[0m:\u001b[94m1999\u001b[0m in \u001b[92marray_cast\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1996 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1997 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m pa.types.is_null(pa_type) \u001b[95mand\u001b[0m \u001b[95mnot\u001b[0m pa.types.is_null(array.t \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1998 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mTypeError\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mCouldn\u001b[0m\u001b[33m'\u001b[0m\u001b[33mt cast array of type \u001b[0m\u001b[33m{\u001b[0marray.type \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1999 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m array.cast(pa_type)                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2000 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mTypeError\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mCouldn\u001b[0m\u001b[33m'\u001b[0m\u001b[33mt cast array of type\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m{\u001b[0marray.type\u001b[33m}\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33mto\u001b[0m\u001b[33m\\n\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2001 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2002 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m in \u001b[92mpyarrow.lib.Array.cast\u001b[0m:\u001b[94m919\u001b[0m                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/pyarrow/\u001b[0m\u001b[1;33mcompute.py\u001b[0m:\u001b[94m389\u001b[0m in \u001b[92mcast\u001b[0m       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m386 \u001b[0m\u001b[2m│   │   │   \u001b[0moptions = CastOptions.unsafe(target_type)                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m387 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m388 \u001b[0m\u001b[2m│   │   │   \u001b[0moptions = CastOptions.safe(target_type)                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m389 \u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m call_function(\u001b[33m\"\u001b[0m\u001b[33mcast\u001b[0m\u001b[33m\"\u001b[0m, [arr], options)                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m390 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m391 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m392 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mindex\u001b[0m(data, value, start=\u001b[94mNone\u001b[0m, end=\u001b[94mNone\u001b[0m, *, memory_pool=\u001b[94mNone\u001b[0m):     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m in \u001b[92mpyarrow._compute.call_function\u001b[0m:\u001b[94m560\u001b[0m                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m in \u001b[92mpyarrow._compute.Function.call\u001b[0m:\u001b[94m355\u001b[0m                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m in \u001b[92mpyarrow.lib.pyarrow_internal_check_status\u001b[0m:\u001b[94m144\u001b[0m                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m in \u001b[92mpyarrow.lib.check_status\u001b[0m:\u001b[94m100\u001b[0m                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[1;91mArrowInvalid: \u001b[0mFailed to parse string: \u001b[32m'1,44477E+18'\u001b[0m as a scalar of type int64\n",
            "\n",
            "\u001b[3mThe above exception was the direct cause of the following exception:\u001b[0m\n",
            "\n",
            "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/drive/MyDrive/Colab Notebooks/clef-23/\u001b[0m\u001b[1;33mrun_glue_v1.py\u001b[0m:\u001b[94m483\u001b[0m in         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m<module>\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m480 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m481 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m482 \u001b[0m\u001b[94mif\u001b[0m \u001b[91m__name__\u001b[0m == \u001b[33m\"\u001b[0m\u001b[33m__main__\u001b[0m\u001b[33m\"\u001b[0m:                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m483 \u001b[2m│   \u001b[0mmain()                                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m484 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/drive/MyDrive/Colab Notebooks/clef-23/\u001b[0m\u001b[1;33mrun_glue_v1.py\u001b[0m:\u001b[94m254\u001b[0m in \u001b[92mmain\u001b[0m    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m251 \u001b[0m\u001b[2m│   │   \u001b[0mlogger.info(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mload a local file for \u001b[0m\u001b[33m{\u001b[0mkey\u001b[33m}\u001b[0m\u001b[33m: \u001b[0m\u001b[33m{\u001b[0mdata_files[key]\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m) \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m252 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m253 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Loading a dataset from local csv files\u001b[0m                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m254 \u001b[2m│   \u001b[0mraw_datasets = load_dataset(                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m255 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mcsv\u001b[0m\u001b[33m\"\u001b[0m,                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m256 \u001b[0m\u001b[2m│   │   \u001b[0mdata_files=data_files,                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m257 \u001b[0m\u001b[2m│   │   \u001b[0mcache_dir=model_args.cache_dir,                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/datasets/\u001b[0m\u001b[1;33mload.py\u001b[0m:\u001b[94m1797\u001b[0m in             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mload_dataset\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1794 \u001b[0m\u001b[2m│   \u001b[0mtry_from_hf_gcs = path \u001b[95mnot\u001b[0m \u001b[95min\u001b[0m _PACKAGED_DATASETS_MODULES          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1795 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1796 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Download and prepare data\u001b[0m                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1797 \u001b[2m│   \u001b[0mbuilder_instance.download_and_prepare(                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1798 \u001b[0m\u001b[2m│   │   \u001b[0mdownload_config=download_config,                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1799 \u001b[0m\u001b[2m│   │   \u001b[0mdownload_mode=download_mode,                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1800 \u001b[0m\u001b[2m│   │   \u001b[0mverification_mode=verification_mode,                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/datasets/\u001b[0m\u001b[1;33mbuilder.py\u001b[0m:\u001b[94m890\u001b[0m in           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mdownload_and_prepare\u001b[0m                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 887 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0mprepare_split_kwargs[\u001b[33m\"\u001b[0m\u001b[33mmax_shard_size\u001b[0m\u001b[33m\"\u001b[0m] =  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 888 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[94mif\u001b[0m num_proc \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 889 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0mprepare_split_kwargs[\u001b[33m\"\u001b[0m\u001b[33mnum_proc\u001b[0m\u001b[33m\"\u001b[0m] = num_pr \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 890 \u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._download_and_prepare(                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 891 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0mdl_manager=dl_manager,                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 892 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0mverification_mode=verification_mode,      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 893 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0m**prepare_split_kwargs,                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/datasets/\u001b[0m\u001b[1;33mbuilder.py\u001b[0m:\u001b[94m985\u001b[0m in           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_download_and_prepare\u001b[0m                                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 982 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 983 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 984 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# Prepare split will record examples associated to th\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 985 \u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._prepare_split(split_generator, **prepare_split_ \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 986 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mOSError\u001b[0m \u001b[94mas\u001b[0m e:                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 987 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mOSError\u001b[0m(                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 988 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mCannot find data file. \u001b[0m\u001b[33m\"\u001b[0m                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/datasets/\u001b[0m\u001b[1;33mbuilder.py\u001b[0m:\u001b[94m1746\u001b[0m in          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_prepare_split\u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1743 \u001b[0m\u001b[2m│   │   │   \u001b[0mgen_kwargs = split_generator.gen_kwargs                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1744 \u001b[0m\u001b[2m│   │   │   \u001b[0mjob_id = \u001b[94m0\u001b[0m                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1745 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m pbar:                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1746 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mfor\u001b[0m job_id, done, content \u001b[95min\u001b[0m \u001b[96mself\u001b[0m._prepare_split_sing \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1747 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mgen_kwargs=gen_kwargs, job_id=job_id, **_prepare_ \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1748 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m):                                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1749 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mif\u001b[0m done:                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/datasets/\u001b[0m\u001b[1;33mbuilder.py\u001b[0m:\u001b[94m1891\u001b[0m in          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_prepare_split_single\u001b[0m                                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1888 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Ignore the writer's error for no examples written to th\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1889 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(e, SchemaInferenceError) \u001b[95mand\u001b[0m e.__context__  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1890 \u001b[0m\u001b[2m│   │   │   │   \u001b[0me = e.__context__                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1891 \u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m DatasetGenerationError(\u001b[33m\"\u001b[0m\u001b[33mAn error occurred while gen\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1892 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1893 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94myield\u001b[0m job_id, \u001b[94mTrue\u001b[0m, (total_num_examples, total_num_bytes, wri \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1894 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[1;91mDatasetGenerationError: \u001b[0mAn error occurred while generating the dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating Classification Report"
      ],
      "metadata": {
        "id": "nah9N1rs6r_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def calculate_performance(y_true, y_pred, labels):\n",
        "    \"\"\"\n",
        "    Calculating performances of our model\n",
        "    :param y_true: actual labels in test set\n",
        "    :param y_pred: predicted labels\n",
        "    :param labels:\n",
        "    :return: accuracy, precision, recall, f1 score and classification report\n",
        "    \"\"\"\n",
        "    (acc, P, R, F1) = (0.0, 0.0, 0.0, 0.0)\n",
        "    acc = metrics.accuracy_score(y_true, y_pred)\n",
        "    P = metrics.precision_score(y_true, y_pred, average='weighted')\n",
        "    R = metrics.recall_score(y_true, y_pred, average='weighted')\n",
        "    F1 = metrics.f1_score(y_true, y_pred, average='weighted')\n",
        "    report = metrics.classification_report(y_true, y_pred, target_names=labels)\n",
        "\n",
        "    return acc * 100, P * 100, R * 100, F1 * 100, report"
      ],
      "metadata": {
        "id": "TZaQ9GubagyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reading files for calculate the performance on test data"
      ],
      "metadata": {
        "id": "Zi1bMng3BHKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "def read_labels(filename):\n",
        "  labels = []\n",
        "  # checking the file is CSV/TXT\n",
        "  if filename.endswith('.csv'):\n",
        "    with open(filename, 'r') as f:\n",
        "      reader = csv.reader(f)\n",
        "      next(reader)\n",
        "      for row in reader:\n",
        "        labels.append(\"Yes\" if row[1]=='1' else \"No\")\n",
        "  else:\n",
        "    lines = open(filename, 'r').read().split(\"\\n\")\n",
        "    for line in lines[1:len(lines)-1]:\n",
        "      lab = line.split(\"\\t\")[1]\n",
        "      labels.append(\"Yes\" if lab=='1' else \"No\")\n",
        "  return labels"
      ],
      "metadata": {
        "id": "XU8zaUyD7tg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Performances on Test set"
      ],
      "metadata": {
        "id": "HZLgQdpuBQ5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_true = read_labels('./data/1B_EN/test.csv')\n",
        "y_test_pred = read_labels('./xlm-roberta-large-output/predict_results.txt')\n",
        "assert len(y_test_true) == len(y_test_pred)\n",
        "target_labels = [\"No\",\"Yes\"]\n",
        "acc, precision, recall, F1, report = calculate_performance(y_test_true, y_test_pred, target_labels)\n",
        "result = str(\"{0:.4f}\".format(acc)) + \"\\t\" + str(\"{0:.4f}\".format(precision)) + \"\\t\" + str(\n",
        "    \"{0:.4f}\".format(recall)) + \"\\t\" + str(\"{0:.4f}\".format(F1)) + \"\\n\"\n",
        "\n",
        "print(\"Test set:\\t Acc\\tPrecision\\tRecall\\tF1\\n\" + result)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "qOhlPRfW7i2x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12c2dbad-114f-4adb-c832-a895045dcfd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set:\t Acc\tPrecision\tRecall\tF1\n",
            "26.4151\t100.0000\t26.4151\t41.7910\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          No       0.00      0.00      0.00         0\n",
            "         Yes       1.00      0.26      0.42       318\n",
            "\n",
            "    accuracy                           0.26       318\n",
            "   macro avg       0.50      0.13      0.21       318\n",
            "weighted avg       1.00      0.26      0.42       318\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(y_test_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ivk87NR7TTH",
        "outputId": "64c0a8ec-634e-4381-a6ba-b77bccc6fe03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "318\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./data/1B_EN/CT23_1B_checkworthy_english_test.tsv') as f:\n",
        "  reader = csv.reader(f, delimiter='\\t')\n",
        "  next(reader)\n",
        "  data = [['Sentence_id', 'Text', 'class_label']]\n",
        "  i = 0\n",
        "  for row in reader:\n",
        "    data.append([row[0], row[1], y_test_pred[i]])\n",
        "    i+= 1\n",
        "\n",
        "\n",
        "with open('./data/1B_EN/subtask1B_english.tsv', 'w') as f:\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    for row in data:\n",
        "      writer.writerow(row)"
      ],
      "metadata": {
        "id": "ynFhwzuI9mty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cm949rMw61z3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}